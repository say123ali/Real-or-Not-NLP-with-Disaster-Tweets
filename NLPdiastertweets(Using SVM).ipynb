{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real or Not? NLP with Disaster Tweets\n",
    "\n",
    "\n",
    "\n",
    "## Competition \n",
    "\n",
    "In this competition, we want to predict which Tweets are about real disasters and which ones are not. Then for each tweet (input), we want to know if it refers to a disaster event (output). The output of our system will be a boolean (true or false).\n",
    "\n",
    "1) Explore data.\n",
    "2) Preprocess data\n",
    "3) TF-IDF implementation.\n",
    "4) SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data \n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('F:/nlp fastai/DisasterTweetsNLP1master/data/train.csv')\n",
    "test_data  = pd.read_csv('F:/nlp fastai/DisasterTweetsNLP1master/data/test.csv')\n",
    "\n",
    "# Show some training data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, in our training data, we have 3 features :\n",
    "- Keyword : a particular keyword from the tweet (may be blank)\n",
    "- Location : the location the tweet was sent from (may be blank)\n",
    "- Text : the text of the tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training :\n",
      "Length of the data : 7613\n",
      "id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training :\")\n",
    "print(\"Length of the data :\", len(train_data))\n",
    "# Missing value in the training set\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test :\n",
      "Length of the data : 3263\n",
      "id             0\n",
      "keyword       26\n",
      "location    1105\n",
      "text           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Test :\")\n",
    "print(\"Length of the data :\", len(test_data))\n",
    "# Missing value in the test set\n",
    "print(test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'samples')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distributy if the target \n",
    "target_values = train_data['target'].value_counts()\n",
    "sns.barplot(target_values.index, target_values)\n",
    "plt.gca().set_ylabel('samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing - Normalization\n",
    "\n",
    "In this part, we are going to normalize our data. Firt of all, we are going to **tokenize** our data. We are going to split our phrase into array of word. We need to separate the word and also the ponctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "['our', 'deeds', 'are', 'the', 'reason', 'of', 'this', '#', 'earthquake', 'may', 'allah', 'forgive', 'us', 'all']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Extract all the words\n",
    "tokens = word_tokenize(train_data[\"text\"][0])\n",
    "\n",
    "# Lowercase the words\n",
    "tokens = [word.lower() for word in tokens]\n",
    "\n",
    "print(train_data[\"text\"][0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we are going to remove all the ponctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our', 'deeds', 'are', 'the', 'reason', 'of', 'this', 'earthquake', 'may', 'allah', 'forgive', 'us', 'all']\n"
     ]
    }
   ],
   "source": [
    "# Remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to describe our tweet with word, we need to remove the \"stopwords\". That means all the basic word like 'the', 'you'... These words are present in all documents, so they can't be use to differentiate tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deeds', 'reason', 'earthquake', 'may', 'allah', 'forgive', 'us']\n"
     ]
    }
   ],
   "source": [
    "# Filters - Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get all stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "words = [word for word in words if not word in stop_words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deed', 'reason', 'earthquak', 'may', 'allah', 'forgiv', 'us']\n"
     ]
    }
   ],
   "source": [
    "# Stem Words (Racinisation)\n",
    "# Process of reducins inflected words to their word stem, base or root form.\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in words]\n",
    "\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "#Function for removing URL\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "#Function for removing HTML codes\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "#Function for removing Emojis\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "#Function for removing punctuations\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "train_data['text']=train_data['text'].apply(remove_URL)\n",
    "train_data['text']=train_data['text'].apply(remove_html)\n",
    "train_data['text']=train_data['text'].apply(remove_emoji)\n",
    "train_data['text']=train_data['text'].apply(remove_punct)\n",
    "\n",
    "\n",
    "test_data['text'] = test_data['text'].apply(remove_URL)\n",
    "test_data['text'] = test_data['text'].apply(remove_html)\n",
    "test_data['text'] = test_data['text'].apply(remove_emoji)\n",
    "test_data['text'] = test_data['text'].apply(remove_punct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>preprocess_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, u]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "      <td>[peopl, receiv, wildfir, evacu, order, califor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1   4     NaN      NaN              Forest fire near La Ronge Sask Canada   \n",
       "2   5     NaN      NaN  All residents asked to shelter in place are be...   \n",
       "3   6     NaN      NaN  13000 people receive wildfires evacuation orde...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "   target                                    preprocess_text  \n",
       "0       1   [deed, reason, earthquak, may, allah, forgiv, u]  \n",
       "1       1       [forest, fire, near, la, rong, sask, canada]  \n",
       "2       1  [resid, ask, shelter, place, notifi, offic, ev...  \n",
       "3       1  [peopl, receiv, wildfir, evacu, order, califor...  \n",
       "4       1  [got, sent, photo, rubi, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Get all stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Extract all the words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercase the words\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Remove all tokens that are not alphabetic\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Remove word in the stop word\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "\n",
    "    # Get the root of the word \n",
    "    stemmed = [porter.stem(word) for word in words]\n",
    "    \n",
    "    # Lematize the word\n",
    "    lematized = [lemmatizer.lemmatize(word) for word in stemmed]\n",
    "\n",
    "    return lematized\n",
    "\n",
    "train_data[\"preprocess_text\"] = train_data.text.apply(preprocess_text)\n",
    "test_data[\"preprocess_text\"] = test_data.text.apply(preprocess_text)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_list(tab):\n",
    "    return \" \".join(tab)\n",
    "train_data[\"text_preprocessed\"] = train_data[\"preprocess_text\"].apply(join_list)\n",
    "test_data[\"text_preprocessed\"] = test_data[\"preprocess_text\"].apply(join_list)\n",
    "\n",
    "def transform_keyword(word) :\n",
    "    # Split when %20\n",
    "    return word.split('%20')\n",
    "\n",
    "# Transform NaN value to empty string\n",
    "train_data[\"keyword\"] = train_data.keyword.fillna(\" \")\n",
    "test_data[\"keyword\"] = test_data.keyword.fillna(\" \")\n",
    "\n",
    "train_data[\"keyword\"] = train_data[\"keyword\"].apply(transform_keyword).apply(join_list)\n",
    "test_data[\"keyword\"] = test_data[\"keyword\"].apply(transform_keyword).apply(join_list)\n",
    "\n",
    "# Concant keyword to the phrases\n",
    "train_data[\"text_preprocessed\"] = train_data[\"keyword\"] + \" \" + train_data[\"text_preprocessed\"] \n",
    "test_data[\"text_preprocessed\"] = test_data[\"keyword\"] + \" \" + test_data[\"text_preprocessed\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_all = pd.concat([train_data[\"text_preprocessed\"], test_data[\"text_preprocessed\"]])\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "tfidf.fit(X_all)\n",
    "\n",
    "X = tfidf.transform(train_data[\"text_preprocessed\"])\n",
    "X_test = tfidf.transform(test_data[\"text_preprocessed\"])\n",
    "del X_all\n",
    "\n",
    "train, test = train_test_split(train_data, test_size=0.2)\n",
    "\n",
    "train_x = train[\"text_preprocessed\"]\n",
    "train_y = train[\"target\"]\n",
    "\n",
    "test_x = test[\"text_preprocessed\"]\n",
    "test_y = test[\"target\"]\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, train_data[\"target\"], test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { \n",
    "    'gamma': [0.001, 0.01, 0.1, 0.4, 0.5, 0.6, 0.7, 1], \n",
    "    'kernel': ['rbf'], \n",
    "    'C': [0.001, 0.01, 0.1, 1, 1.5, 2, 3, 10],\n",
    "}\n",
    "\n",
    "# {'C': 2, 'gamma': 0.9, 'kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = { \n",
    "#     'gamma':  [0.5],\n",
    "#     'kernel': ['rbf'], \n",
    "#     'C':[2]\n",
    "# }\n",
    "model = GridSearchCV(SVC(), parameters, cv=10, n_jobs=-1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.5, 'gamma': 0.6, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cv_results_['params'][model.best_index_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7821522309711286, 0.7242524916943522)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_pred = model.predict(X_val)\n",
    "accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[378,  48],\n",
       "       [118, 218]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv( 'F:/nlp fastai/DisasterTweetsNLP1master/data/sample_submission.csv')\n",
    "sub_df[\"target\"] = y_test_pred\n",
    "sub_df.to_csv(\"F:/nlp fastai/DisasterTweetsNLP1master/submission3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
